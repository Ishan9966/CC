{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK resources \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Artificial Intelligence is transforming various industries, making tasks easier and more efficient. \n",
    "From healthcare to finance, AI-powered systems are becoming indispensable tools for decision-making. \n",
    "Technologies like deep learning and machine learning allow for better predictions, automated processes, and enhanced data analysis.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Convert text to lowercase and remove punctuation\n",
    "text_lower = text.lower()\n",
    "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Cleaned Text:\")\n",
    "print(text_clean)\n",
    "\n",
    "# 2. Tokenize the text into words and sentence\n",
    "words = word_tokenize(text_clean)\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"\\nTokens:\")\n",
    "print(words)\n",
    "\n",
    "# 3. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(\"\\nFiltered Words:\")\n",
    "print(filtered_words)\n",
    "\n",
    "# 4. Display word frequency distribution\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"Word Frequency Distribution (Excluding Stopwords):\")\n",
    "print(word_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PorterStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize stemmers and lemmatizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m porter_stemmer = \u001b[43mPorterStemmer\u001b[49m()\n\u001b[32m      3\u001b[39m lancaster_stemmer = LancasterStemmer()\n\u001b[32m      4\u001b[39m lemmatizer = WordNetLemmatizer()\n",
      "\u001b[31mNameError\u001b[39m: name 'PorterStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize stemmers and lemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply stemming using PorterStemmer and LancasterStemmer\n",
    "porter_stemmed = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "# Apply lemmatization using WordNetLemmatizer\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "# Compare and display results of both techniques\n",
    "print(\"\\nStemming (PorterStemmer):\", porter_stemmed)\n",
    "print(\"\\nStemming (LancasterStemmer):\", lancaster_stemmed)\n",
    "print(\"\\nLemmatization:\", lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use regular expressions to extract:\n",
    "# a. All words with more than 5 letters\n",
    "long_words = re.findall(r'\\b\\w{6,}\\b', text_clean)\n",
    "print(\"\\nWords with more than 5 letters:\", long_words)\n",
    "\n",
    "# b. All numbers \n",
    "numbers = re.findall(r'\\b\\d+\\b', text_clean)\n",
    "print(\"\\nNumbers found in text:\", numbers)\n",
    "\n",
    "# c. All capitalized words\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text_clean)\n",
    "print(\"\\nCapitalized words:\", capitalized_words)\n",
    "\n",
    "# 2. Use text splitting techniques:\n",
    "# a. Split the text into words containing only alphabets \n",
    "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', text_clean)\n",
    "print(\"\\nAlphabetic words:\", alphabetic_words)\n",
    "\n",
    "# b. Extract words starting with a vowel\n",
    "vowel_words = re.findall(r'\\b[aeiouAEIOU]\\w*\\b', text_clean)\n",
    "print(\"\\nWords starting with a vowel:\", vowel_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Custom tokenization function\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^\\w\\s\\'-]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "custom_tokens = custom_tokenizer(text)\n",
    "print(\"\\nCustom Tokenized Text:\", custom_tokens)\n",
    "\n",
    "# 2. Regex substitutions for cleaning\n",
    "# a. Replace email addresses with '<EMAIL>'\n",
    "text_with_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '<EMAIL>', text)\n",
    "# b. Replace URLs with '<URL>'\n",
    "text_with_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text_with_emails)\n",
    "# c. Replace phone numbers with '<PHONE>'\n",
    "text_cleaned = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_with_urls)\n",
    "\n",
    "print(\"\\nText with Emails, URLs, and Phone Numbers Replaced:\")\n",
    "print(text_cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
